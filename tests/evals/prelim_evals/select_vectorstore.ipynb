{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which vectorstore should I use?\n",
    "\n",
    "CONCLUSION - going without for now...I need a more fine grained approach in terms of how I load/parse pdfs and extract what I need...also I'd like to store what I need in a more structured way because llms aren't answering questions from these docs reliably enough for a demo-able product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.document import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "from meche_copilot.chains.helpers.specs_retriever import SpecsRetriever\n",
    "from meche_copilot.utils.envars import OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load documents using langchain pdf loader\n",
    "ref_docs = [\"demo-01/engineering_design_drawings.pdf\"]\n",
    "\n",
    "documents = []\n",
    "for fpath in ref_docs:\n",
    "  loader = PyPDFLoader(str(fpath))\n",
    "  documents = documents + loader.load()\n",
    "\n",
    "# len(documents) #36 each page is a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create llm\n",
    "llm = ChatOpenAI(temperature=0, openai_api_key=OPENAI_API_KEY, model=\"gpt-4\")\n",
    "openai_embedding_func = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vectorstore\n",
    "chroma_db = Chroma(\n",
    "  embedding_function=openai_embedding_func,\n",
    "  persist_directory=\"data/.chroma_db\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add docs to the vectorstore\n",
    "chroma_db.add_documents(documents) # this adds duplicates, search for existing first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can delete by path? yes\n",
    "ids = chroma_db.get(\n",
    "  where={\"source\": \"demo-01/engineering_design_drawings.pdf\"}\n",
    ")['ids']\n",
    "chroma_db.delete(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can get by page?\n",
    "res = chroma_db.get(\n",
    "  where={\"page\": 35}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specs retriever get relavent docs for specs lookup?\n",
    "\n",
    "retriever_llm = ChatOpenAI(temperature=0, openai_api_key=OPENAI_API_KEY, model=\"gpt-4\")\n",
    "specs_retriever = SpecsRetriever(llm=retriever_llm, vectorstore=chroma_db, source=self.source)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CUSTOM VECTORSTORE ###\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from numpy import char\n",
    "import pandas as pd\n",
    "# from esd_copilot.esd_toolkit.schemas import Esd, EsdConfig\n",
    "# from esd_copilot.utils.config import load_config, find_config\n",
    "\n",
    "# esd_config_box = load_config(find_config('esd-config.yaml'))\n",
    "# esd_config = EsdConfig(**esd_config_box)\n",
    "# esd = Esd.from_config(config=esd_config)\n",
    "\n",
    "# # fan eq test\n",
    "# eq = esd.equipments[2]\n",
    "# char_desc_dict = eq.char_descs\n",
    "# spec_results_dict = {}\n",
    "# spec_results_dict['spec description'] = char_desc_dict\n",
    "# for i, inst in enumerate(eq.instances):\n",
    "#     char_result_dict = {key: \"(SPEC, PAGE)\" for key in char_desc_dict.keys()}\n",
    "#     # name = inst.inst_name\n",
    "#     # name = name.replace(\"template\", \"\").replace(\" \", \"\")\n",
    "#     spec_results_dict[inst.inst_name] = char_result_dict\n",
    "\n",
    "# pd.DataFrame(spec_results_dict)\n",
    "    \n",
    "# Document loader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"demo-01/engineering_design_drawings.pdf\")\n",
    "data = loader.load()\n",
    "\n",
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 0)\n",
    "all_splits = text_splitter.split_documents(data)\n",
    "\n",
    "# Store \n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from meche_copilot.utils.envars import OPENAI_API_KEY\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=all_splits,embedding=OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY))\n",
    "\n",
    "# Retriever\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(retriever=vectorstore.as_retriever(), llm=ChatOpenAI(temperature=0, openai_api_key=OPENAI_API_KEY, max_tokens=5000))\n",
    "\n",
    "# Run chain\n",
    "from langchain.chains import RetrievalQA\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "# chain_type_kwargs = {\"prompt\": prompt}\n",
    "qa_chain = RetrievalQA.from_chain_type(llm, retriever=retriever_from_llm)\n",
    "\n",
    "unique_docs = retriever_from_llm.get_relevant_documents(query=\"Get the whole contents of the exhaust fan schedule table\")\n",
    "\n",
    "example_spec_results_dict = {\n",
    "    \"spec description\": {\n",
    "        \"spec1\": \"spec1 is an example spec\",\n",
    "        \"spec2\": \"spec2 is another example spec\",\n",
    "    },\n",
    "    \"instance1\": {\n",
    "        \"spec1\": \"(SPEC, PAGE)\",\n",
    "        \"spec2\": \"(SPEC, PAGE)\",\n",
    "    },\n",
    "    \"instance2\": {\n",
    "        \"spec1\": \"(SPEC, PAGE)\",\n",
    "        \"spec2\": \"(SPEC, PAGE)\",\n",
    "    },\n",
    "}\n",
    "\n",
    "template = \"\"\"\n",
    "Your job is to fillout specs (short for specifications) and return a valid json blob.\n",
    "\n",
    "To do this, replace '(SPEC, PAGE)' in the specs_json_blob with the specs you find in the reference documents or '(UNK, UNK)' if you can't find the spec that matches the spec description.\n",
    "\n",
    "Here is an example of a valid $JSON_BLOB:\n",
    "```json\n",
    "    \"answer\": {example_spec_results_dict},\n",
    "```\n",
    "Reference Documents: {context}\n",
    "\n",
    "specs_json_blob: {spec_results_dict}\n",
    "\n",
    "ONLY RESPOND WITH your updated specs_json_blob as a valid json blob like this:\n",
    "```json\n",
    "specs_json_blob: your answer here\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template=template)\n",
    "INPUT_PROMPT = prompt.format(context=unique_docs, \n",
    "spec_results_dict=spec_results_dict, example_spec_results_dict=example_spec_results_dict)\n",
    "\n",
    "print(INPUT_PROMPT)\n",
    "\n",
    "result = qa_chain({\"query\": INPUT_PROMPT})\n",
    "res = result['result']\n",
    "\n",
    "import ast\n",
    "\n",
    "# Extract the dictionary string\n",
    "dict_string = res.split(\"specification_results: \")[1].split(\"\\n\\nReplacing\")[0]\n",
    "# Convert the string to a dictionary\n",
    "dict_result = ast.literal_eval(dict_string)\n",
    "pd.DataFrame(dict_result)\n",
    "\n",
    "print(dict_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### AUTOCREATE VECTORSTORE ###\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader, PDFMinerLoader, PDFPlumberLoader, PyPDFDirectoryLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "\n",
    "from meche_copilot.utils.envars import OPENAI_API_KEY\n",
    "\n",
    "######## Autocreate vectorstore from docs ########\n",
    "# Document loader\n",
    "loader = PyPDFLoader(\"demo-01/engineering_design_drawings.pdf\")\n",
    "# Index that wraps above steps\n",
    "kw_args = {\"openai_api_key\": OPENAI_API_KEY}\n",
    "index = VectorstoreIndexCreator(vectorstore_kwargs=kw_args).from_loaders([loader])\n",
    "# Question-answering\n",
    "question = \"What is on page two of the engineering design drawings?\"\n",
    "index.query(question)\n",
    "\n",
    "\n",
    "####### Retrieval QA with Sources chain ########\n",
    "\n",
    "# Split (use Grobid for context aware splitting)\n",
    "# loader uses GROBIB to parse PDFs into Documents that retain metadata associated with the section of text.\n",
    "from langchain.document_loaders.parsers import GrobidParser\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "\n",
    "# Document loader\n",
    "loader = GenericLoader.from_filesystem(\n",
    "    \"demo-01/\",\n",
    "    glob=\"engineering_design_drawings.pdf\",\n",
    "    suffixes=[\".pdf\"],\n",
    "    parser=GrobidParser(segment_sentences=False),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# Document loader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"demo-01/engineering_design_drawings.pdf\")\n",
    "data = loader.load()\n",
    "\n",
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 0)\n",
    "all_splits = text_splitter.split_documents(data)\n",
    "\n",
    "# Store \n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from meche_copilot.utils.envars import OPENAI_API_KEY\n",
    "vectorstore = Chroma.from_documents(documents=all_splits,embedding=OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY))\n",
    "\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(retriever=vectorstore.as_retriever(), llm=ChatOpenAI(temperature=0))\n",
    "\n",
    "unique_docs = retriever_from_llm.get_relevant_documents(query='What is in the exhaust fan schedule?')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meche-copilot-py-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
